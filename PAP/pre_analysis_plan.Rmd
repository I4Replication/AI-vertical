---
title: "Bridging Research Gaps With AI: Expertise and Disciplinary Mobility"
author: "\\begin{minipage}{0.9\\textwidth}\\centering \\small Ghina Abdul Baki, Juan P. Aparicio, Bruno Barbarioli, Abel Brodeur,\\\\ Lenka Fiala, Derek Mikola, David Valenta \\end{minipage}"
date: "`r format(Sys.Date())`"
output:
  pdf_document:
    toc: false
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
bibliography: references.bib
link-citations: true
fontsize: 11pt
geometry: margin=1in
header-includes:
  - \usepackage{adjustbox}
  - \usepackage{float}
  - \usepackage{placeins}
---

```{r setup, include=FALSE}
Sys.setenv(
  OMP_NUM_THREADS = "1",
  OPENBLAS_NUM_THREADS = "1",
  MKL_NUM_THREADS = "1",
  MKL_THREADING_LAYER = "SEQUENTIAL",
  KMP_DUPLICATE_LIB_OK = "TRUE"
)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = 'center',
  fig.width = 7,
  fig.height = 4.5
)

# helper to sanitize figure paths (avoid spaces/parentheses issues in LaTeX)
sanitize_name <- function(x) {
  x <- gsub("[ ()]", "_", x)
  gsub("__+", "_", x)
}
ensure_sanitized_figures <- function(files) {
  dir.create('output/figures_sanitized', showWarnings = FALSE, recursive = TRUE)
  src <- file.path('output/figures', files)
  dst <- file.path('output/figures_sanitized', sanitize_name(files))
  ok <- file.exists(src)
  if (any(!ok)) warning('Missing figures: ', paste(files[!ok], collapse=', '))
  for (i in which(ok)) {
    if (!file.exists(dst[i]) || file.mtime(src[i]) > file.mtime(dst[i])) {
      invisible(file.copy(src[i], dst[i], overwrite = TRUE))
    }
  }
  dst[ok]
}

# helper to inject LaTeX caption/label into external .tex tables
emit_table_with_caption <- function(tex_path, caption, label) {
  x <- readLines(tex_path, warn = FALSE)
  has_table <- any(grepl(r"(^\\begin\{table\})", x))
  if (!has_table) {
    # Wrap raw tabular/tblr into a floating table with caption/label and scaling
    j_start <- which(grepl(r"(^\\begin\{tblr\})", x))[1]
    j_end   <- which(grepl(r"(^\\end\{tblr\})", x))[1]
    if (is.na(j_start) || is.na(j_end)) {
      j_start <- which(grepl(r"(^\\begin\{tabular\})", x))[1]
      j_end   <- which(grepl(r"(^\\end\{tabular\})", x))[1]
    }
    if (is.na(j_start) || is.na(j_end)) stop('Expected a tabular or tblr in ', tex_path)
    pre  <- if (j_start > 1) x[seq_len(j_start-1)] else character(0)
    mid  <- x[j_start:j_end]
    post <- if (j_end < length(x)) x[(j_end+1):length(x)] else character(0)
    y <- c(
      "\\begin{table}[H]",
      "\\centering",
      "\\begingroup\\scriptsize",
      paste0("\\caption{", caption, "}\\label{", label, "}"),
      "\\begin{adjustbox}{width=\\textwidth,totalheight=0.8\\textheight,keepaspectratio}",
      pre,
      mid,
      post,
      "\\end{adjustbox}",
      "\\endgroup",
      "\\end{table}"
    )
    cat(y, sep='\n')
    return(invisible(NULL))
  }
  # Strengthen float placement and add caption/label
  i_tbl <- which(grepl(r"(^\\begin\{table\})", x))[1]
  if (!is.na(i_tbl)) x[i_tbl] <- "\\begin{table}[H]"
  has_caption <- any(grepl(r"(^\\caption)", x))
  if (!has_caption) {
    ins <- paste0('\\caption{', caption, '}\\label{', label, '}')
    x <- append(x, values = ins, after = i_tbl)
  }
  # Add small font group after centering if present
  i_ctr <- which(grepl(r"(^\\centering$)", x))[1]
  if (!is.na(i_ctr)) x <- append(x, values = "\\begingroup\\scriptsize", after = i_ctr)
  # Wrap inner table
  j_start <- which(grepl(r"(^\\begin\{tblr\})", x))[1]
  j_end   <- which(grepl(r"(^\\end\{tblr\})", x))[1]
  if (is.na(j_start) || is.na(j_end)) {
    j_start <- which(grepl(r"(^\\begin\{tabular\})", x))[1]
    j_end   <- which(grepl(r"(^\\end\{tabular\})", x))[1]
  }
  if (!is.na(j_start) && !is.na(j_end) && j_end > j_start) {
    x <- append(x, values = "\\begin{adjustbox}{width=\\textwidth,totalheight=0.8\\textheight,keepaspectratio}", after = j_start - 1)
    j_end <- j_end + 1
    x <- append(x, values = "\\end{adjustbox}", after = j_end)
  }
  i_end_tbl <- which(grepl(r"(^\\end\{table\})", x))[1]
  if (!is.na(i_end_tbl)) x <- append(x, values = "\\endgroup", after = i_end_tbl - 1)
  cat(x, sep='\n')
}

# AI-paper style table helpers for use inside this Rmd (classic tabular)
suppressPackageStartupMessages({library(broom)})
fmt3 <- function(x) { if (is.na(x) || is.null(x)) return("—"); sprintf("% .3f", x) }
star_sym <- function(p) { if (is.na(p)) return(""); if (p < 0.01) "***" else if (p < 0.05) "**" else if (p < 0.10) "*" else "" }
coef_stats <- function(m, term) {
  tt <- tryCatch(broom::tidy(m, conf.int = TRUE), error = function(e) NULL)
  if (is.null(tt)) return(list(est=NA,se=NA,p=NA,lwr=NA,upr=NA))
  row <- tt[tt$term == term, , drop = FALSE]
  if (nrow(row) == 0 && grepl(':', term, fixed = TRUE)) {
    parts <- strsplit(term, ':', fixed = TRUE)[[1]]
    term2 <- paste(rev(parts), collapse = ':')
    row <- tt[tt$term == term2, , drop = FALSE]
  }
  if (nrow(row) == 0 && grepl(':', term, fixed = TRUE)) {
    req <- tolower(strsplit(term, ':', fixed = TRUE)[[1]])
    req <- trimws(req)
    tokenise <- function(x) unique(trimws(strsplit(tolower(x), "[^A-Za-z0-9_]+")[[1]]))
    has_all <- function(trm) {
      toks <- tokenise(trm)
      all(req %in% toks)
    }
    idx <- which(vapply(tt$term, has_all, logical(1)))
    if (length(idx) >= 1) row <- tt[idx[1], , drop = FALSE]
  }
  if (nrow(row) == 0) return(list(est=NA,se=NA,p=NA,lwr=NA,upr=NA))
  list(est = row$estimate[[1]], se = row$std.error[[1]], p = row$p.value[[1]], lwr = row$conf.low[[1]], upr = row$conf.high[[1]])
}
wald_compression_p <- function(m) {
  tt <- tryCatch(broom::tidy(m), error = function(e) NULL)
  if (is.null(tt)) return(NA_real_)
  cand <- unique(c(tt$term[grepl('^treatment:tier', tt$term)], tt$term[grepl('^tier.*:treatment$', tt$term)]))
  if (length(cand) <= 1) return(NA_real_)
  base <- cand[[1]]; rest <- setdiff(cand, base)
  R <- paste(base, '=', rest)
  tryCatch({ fixest::wald(m, R)$p.value }, error = function(e) NA_real_)
}
write_ai_table <- function(models, col_titles, file, coef_mode = c('tier','years','learn','ood'),
                           controls_desc = 'Event×article FE; years of coding; software; prior AI familiarity.',
                           dep_means = NULL) {
  coef_mode <- match.arg(coef_mode)
  K <- length(models); stopifnot(length(col_titles) == K)
  dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)
  if (coef_mode == 'tier') {
    keep <- c('treatment','treatment:tier_3GR','treatment:tier_3PR')
    labels <- c('AI-Assisted','AI × Graduate','AI × Professor/Researcher')
  } else if (coef_mode == 'years') {
    keep <- c('treatment','years_coding:treatment')
    labels <- c('AI-Assisted','AI × Years of coding')
  } else if (coef_mode == 'learn') {
    keep <- c('treatment','treatment:event_order')
    labels <- c('AI-Assisted','AI × Event order')
  }
  lines <- c()
  lines <- c(lines, '\\def\\sym#1{\\ifmmode^{#1}\\else\\(^{#1}\\)\\fi}')
  lines <- c(lines, sprintf('\\begin{tabular}{l*{%d}{c}}', length(models)))
  lines <- c(lines, '\\hline\\hline')
  # Numbers first, then titles
  lines <- c(lines, paste(c('', sprintf('(%d)', seq_len(K))), collapse=' & '), '\\\\')
  lines <- c(lines, paste(c('', col_titles), collapse=' & '), ' \\\\')
  lines <- c(lines, '\\hline')
  for (i in seq_along(keep)) {
    lab <- labels[[i]]
    ests <- se_line <- ci_line <- character(K)
    for (j in seq_len(K)) {
      s <- coef_stats(models[[j]], keep[[i]])
      ests[[j]] <- if (is.na(s$est)) '—' else sprintf('%s%s', fmt3(s$est), star_sym(s$p))
      se_line[[j]] <- if (is.na(s$se)) '' else sprintf('(%s)', fmt3(s$se))
      ci_line[[j]] <- if (is.na(s$lwr) || is.na(s$upr)) '' else sprintf('[%s; %s]', fmt3(s$lwr), fmt3(s$upr))
    }
    lines <- c(lines, paste(c(lab, ests), collapse=' & '), '\\\\')
    lines <- c(lines, paste(c('', se_line), collapse=' & '), '\\\\')
    lines <- c(lines, paste(c('', ci_line), collapse=' & '), '\\\\')
  }
  lines <- c(lines, '\\hline')
  ck <- rep('$\\checkmark$', K)
  lines <- c(lines, paste(c('Controls', ck), collapse=' & '), '\\\\')
  if (is.null(dep_means)) {
    dep_means <- sapply(models, function(m) {
      val <- tryCatch({as.numeric(fixest::fitstat(m, 'ymean')$value)}, error=function(e) NA_real_)
      if (is.na(val)) val <- tryCatch({mean(m$fitted.values + residuals(m), na.rm=TRUE)}, error=function(e) NA_real_)
      val
    })
  }
  lines <- c(lines, paste(c('Mean of dep. var', sprintf('% .3f', dep_means)), collapse=' & '), '\\\\')
  if (coef_mode == 'tier') {
    # monotonic compression p-values (one-sided)
    p_monotonic_tier <- function(m) {
      b <- tryCatch(coef(m), error = function(e) NULL)
      V <- tryCatch(vcov(m), error = function(e) NULL)
      if (is.null(b) || is.null(V)) return(NA_real_)
      find_idx <- function(name){
        if (name %in% names(b)) return(which(names(b)==name))
        parts <- strsplit(name, ':', fixed = TRUE)[[1]]
        name2 <- paste(rev(parts), collapse=':')
        if (name2 %in% names(b)) return(which(names(b)==name2))
        return(NA_integer_)
      }
      idx_main <- find_idx('treatment')
      idx_GR   <- find_idx('treatment:tier_3GR')
      idx_PR   <- find_idx('treatment:tier_3PR')
      if (any(is.na(c(idx_main, idx_GR, idx_PR)))) return(NA_real_)
      mk_p <- function(i, j){
        cvec <- rep(0, length(b)); cvec[i] <- -1; cvec[j] <- 1
        est <- sum(cvec * b)
        se <- sqrt(as.numeric(t(cvec) %*% V %*% cvec))
        if (!is.finite(se) || se <= 0) return(NA_real_)
        z <- est / se
        1 - pnorm(z)
      }
      p1 <- mk_p(idx_main, idx_GR)
      p2 <- mk_p(idx_GR, idx_PR)
      if (any(is.na(c(p1,p2)))) return(NA_real_)
      max(p1,p2)
    }
    pvals <- sapply(models, function(m) fmt3(tryCatch(p_monotonic_tier(m), error=function(e) NA_real_)))
    lines <- c(lines, paste(c('p-val (Monotonic compression)', pvals), collapse=' & '), '\\\\')
  }
  lines <- c(lines, paste(c('Obs.', sapply(models, nobs)), collapse=' & '), '\\\\')
  lines <- c(lines, '\\hline', '\\hline\\hline')
  ctrl_txt <- gsub('&', '\\&', controls_desc, fixed = TRUE)
  lines <- c(lines,
             paste0('\\multicolumn{', K+1, '}{l}{\\it{Note:} Standard errors in parentheses; confidence intervals in brackets.}\\\\'),
             paste0('\\multicolumn{', K+1, '}{l}{Controls: ', ctrl_txt, '}\\\\'))
  if (coef_mode == 'tier') {
    lines <- c(lines, paste0('\\multicolumn{', K+1, '}{l}{Compression (monotonic): one-sided p-value for increasing effects across the three tiers (baseline: Undergraduate).}\\\\'))
  }
  lines <- c(lines, paste0('\\multicolumn{', K+1, '}{l}{\\sym{*} $p<0.10$, \\sym{**} $p<0.05$,  \\sym{***} $p<0.01$}\\\\'))
  lines <- c(lines, '\\end{tabular}')
  writeLines(lines, con = file)
}

# Build analysis tables before inclusion (regenerates output/tables/* when requested)
if (identical(Sys.getenv("PAP_REBUILD_TABLES"), "1")) {
  try({
    source('code/R code/pap_analyses.R')
  }, silent = TRUE)
}
```

# Abstract

We test whether providing large-language-model (LLM) assistance compresses performance gaps in research skills along two prespecified dimensions. Vertically, we study expertise tiers using a three-category grouping (undergraduate, graduate student, professor/researcher) that governs both randomization and analysis while retaining self-reported titles for descriptive context. Horizontally, we study cross-discipline performance in three quantitative social sciences (Economics, Political Science, Psychology) to assess whether AI reduces the penalty from working outside one’s primary discipline (out-of-discipline, OOD). Participants are randomized 1:1 to AI access (e.g. ChatGPT) versus human-only controls within the applicable expertise strata; tasks are assigned a single discipline tag, with undergraduates always receiving inside-discipline assignments and approximately the remaining participants randomly assigned to an outside-discipline paper. Outcome measures are organized into two families: coding skills (computational reproducibility and the detection of major/minor coding errors) and non-coding skills (referee appropriateness/overall scores and robustness-check execution). The study is designed to reveal whether AI acts as an equalizer within expertise tiers and whether it enables researchers to operate effectively across disciplines.

# Registration and Funding

This pre-analysis plan is preregistered on the Open Science Framework (OSF) at https://osf.io/dkfzt/. Funded by the Social Sciences and Humanities Research Council. The locked PAP together with the analysis scripts used to generate the mock tables and figures are mirrored there; live study code and data will be added only after the analysis lock.

# Background and Rationale

Prior multi‑site “AI Replication Games” documented measurable AI effects on success and speed in reproduction tasks, while also revealing substantial variation across participants and events [@brodeur2025_ai_replication_games]. Building on that foundation, we preregister two complementary dimensions of distributional impacts. First, the vertical dimension (expertise ladder): whether AI narrows performance gaps by disproportionately lifting less‑experienced participants (equalizer) or widens gaps by enabling experts to better leverage tools (amplifier). Second, the horizontal dimension (cross‑discipline): whether AI reduces the penalty from working outside one’s primary discipline (out‑of‑discipline, OOD) when reproducing studies across Economics, Political Science, and Psychology. These questions matter for pedagogy, workforce development, and equity: vertical compression would support broad inclusion strategies; horizontal compression would support cross‑field mobility and knowledge diffusion; in both cases, amplification would call for targeted training and governance to avoid widening disparities.

We keep tasks, instructions, and grading rubrics closely aligned with prior exercises to ensure comparability while tailoring the design to individual-level randomization within strata and a discipline-tagged task pool, the design isolates intent-to-treat effects within expertise tiers and introduces an orthogonal OOD contrast by allocating non-undergraduate participants to an outside-discipline article while keeping all undergraduates inside their primary field. This enables transparent tests of heterogeneous effects across tiers (vertical) and along the OOD dimension (horizontal). To support interpretability, we prespecify a compact outcome set split between coding skills (computational reproducibility; identification of major and minor coding errors) and non-coding skills (referee appropriateness/overall judgments and robustness-check execution) alongside a small list of precision-enhancing controls; discipline fixed effects are nested within article fixed effects and are thus absorbed in all models.

Several design features anchor the study and deserve to be unpacked carefully. We begin with randomization: participants enter one of three expertise strata—undergraduate, graduate (master’s and PhD students together), or professor/researcher (postdocs and faculty). Within each stratum, participants are randomized 1:1 to either AI assistance or a human-only control condition. This stratification keeps comparisons within peer groups while preserving overall balance; it also fixes the reference point for the “vertical” estimands that trace how treatment effects evolve along the expertise ladder.

Task assignment introduces the horizontal dimension. Articles arrive with a single discipline tag (Economics, Political Science, or Psychology), enabling us to contrast performance inside versus outside a participant’s primary field. We explicitly model two regressions—vertical (AI × tier) and horizontal (AI × out-of-discipline)—and we test each dimension on its own set of outcomes. This separation keeps the interpretation of compression tests transparent and avoids conflating discipline moves with tier moves.

Outcome measurement rounds out the design. Coding outcomes combine a binary indicator for successful computational reproducibility with the counts of major and minor coding errors flagged during replication. Non-coding outcomes focus on communication and judgment: human referees—blinded to treatment and both participants and paper discipline—and a parallel AI grader each record whether the submission is appropriate and assign an overall 0–5 score, while participants’ robustness logs record whether they execute at least one or at least two qualifying checks. Section “Grading Rubric” details the anchors that guide human judgments, while Appendix “AI Referee Prompt” reproduces the corresponding instructions for the AI grader, ensuring that communication quality is assessed consistently across arms. We also track participants' robustness checks proposals and implementations as part of the non-coding skill set to contextualize main effects. 

# Research Questions and Hypotheses

This study addresses three linked questions. First, we ask whether AI assistance compresses or widens the expertise gradient by comparing treatment effects across the three pre-defined tiers; this provides the “vertical” lens that motivates the stratified randomization. Second, we investigate whether AI attenuates the penalty from tackling a problem outside one’s primary discipline, thereby capturing “horizontal” mobility across Economics, Political Science, and Psychology. Third, we consider the intent-to-treat contrast averaged across all participants, which anchors the benchmark effect size against which heterogeneous responses are interpreted.

The corresponding hypotheses follow naturally. H1 posits that access to AI increases increases research and coding skills—raising the probability of a successful reproduction while lowering major and minor coding errors. H2 mirrors this logic across disciplines, anticipating that AI shrinks the outside-of-discipline penalty for all outcome variables. H3 posits that AI assistance improves coding and research performance.

Throughout the document we rely on several recurring definitions. “Reproduction success” means the participant’s final output matches the prespecified focal result. “Error detection” counts correctly identified coding errors and distinguishes between major issues—those that would alter the substantive result—and minor issues that affect presentation or reproducibility without changing the significance and magnitude of the estimate. “Referee report quality” focuses on the Appropriateness indicator (binary) and the overall 0–5 score derived from the referee rubric; the underlying rubric is available to judges but only these two statistics enter the preregistered estimands. “Robustness execution” records whether participants implement at least one or at least two robustness checks that meet the prespecified criteria. Finally, “out-of-discipline (OOD)” labels any case in which the task’s discipline tag differs from the participant’s self-reported primary discipline.

# Experimental Design

We recruit participants across the three canonical tiers—undergraduates, graduate students (master’s and PhD), and professors/researchers (postdocs and faculty)—and observe each in a single, timed, one-day session. Randomization always occurs within these three strata while we continue to track individual titles for descriptive reporting. Every participant attempts to reproduce one prespecified result using their preferred software ecosystems (R, Stata, or Python) within a seven-hour working window. The window mirrors prior AI Replication Games, where most participants completed their submissions within seven hours while still allowing careful documentation. We randomly assign access to AI assistance (ChatGPT Plus with tools) within the relevant strata and events. Participants in the control arm pledge not to use AI tools. Human referees are blinded to both treatment and participants and paper discipline, and filenames/metadata that could reveal either are redacted.

Within each event, articles are randomly assigned from a pre-curated pool. once participant rosters are finalized. After the random draw, the research team audits each assigned paper to confirm that (i) the replication package includes executable code together with a README or documentation, (ii) all requisite datasets and intermediate files are accessible without additional permissions, and (iii) the combined data footprint remains tractable for local execution (target: compressed package ≤ 500 MB and memory requirements within a standard 16 GB RAM laptop). Papers that fail any check are replaced before assignments are communicated to participants.

Task materials are version-controlled in `Papers/`, which stores a journal-level folder for each study together with the inventory workbook `Papers/papers.xlsx`. As of the lock, the workbook enumerates 15 studies (American Journal of Political Science = 5, American Economic Review: Applied Economics = 5, Psychological Science = 5) with DOI identifiers and replication-package URLs. Each journal folder contains the published article (`paper.pdf`) and the original replication package supplied by the journal or data repository. This structure lets us share identical bundles across events while keeping provenance and updates transparent.

The design studies vertical compression across expertise and horizontal compression across disciplines in a unified framework (run simultaneously). Each article carries a single discipline tag—Economics, Political Science, or Psychology—and participants self-report a primary discipline at registration. The task pool spans all three fields at every event. Undergraduates always receive inside-discipline papers; among graduate students and professors/researchers (postdocs and faculty) we randomly select individuals to work outside their primary discipline (OOD), with the remainder kept inside. This prioritizes a fixed OOD exposure rate over per-cell balance. Event×article fixed effects absorb site, tooling, and task heterogeneity; because each article maps to a single discipline, separate discipline indicators are redundant.

## Pre-Event Training

Participants attend a 60-minute orientation the week before each event (slides in `Pre game/ai_research_webinar_codex_cli_v2.pdf`). The session focuses on logistics: how the games run, what deliverables to submit, the structure of the Excel tracking file, and expectations about documentation, timing, and referee reports. We provide a brief overview of ChatGPT Plus features so attendees understand the tools that treated participants will receive, but the emphasis is on workflow discipline, reproducibility reminders, and answering procedural questions. Randomization is revealed one day before each event so that ChatGPT Plus invitations can be activated for treated participants in advance.

## Focus Groups

To contextualize quantitative findings, we will run four parallel focus-group sessions (six participants per group) immediately after the first event concludes. Groups are stratified by treatment status (AI-assisted vs. human-only) and, when numbers permit, by inside/outside-discipline assignments so that discussions surface arm-specific workflows and cross-discipline frictions. Each 60-minute session is moderated by Institute for Replication staff using the standardized guide in `focus_groups/focus_group_guide.md`; facilitators remind attendees of confidentiality expectations and collect recorded consent before beginning. Discussion notes and transcripts are coded with the companion qualitative codebook (`focus_groups/codebook.md`), which maps themes on motivations, preparation, AI use or workarounds, cross-discipline challenges, and suggestions for future waves. Insights from these sessions inform protocol refinements before scaling beyond the first event.

Design cells overview (what is crossed with what):

```{r tbl-design-cells, results='asis'}
has_kx <- requireNamespace("kableExtra", quietly = TRUE)
design <- tibble::tribble(
  ~Factor, ~Levels, ~Notes,
  "Arm", "Human-Only; AI-Assisted", "1:1 allocation within tier",
  "Expertise tier", "Undergraduate; Graduate; Professor/Researcher", "Stratification (randomization blocks; Graduate = MA/PhD, Professor/Researcher = postdoc/faculty)",
  "OOD status", "Inside; Outside", "Derived from participant vs task discipline"
)
if (has_kx) {
  kableExtra::kbl(design, booktabs = TRUE, caption = "Design factors and levels (crossed: Arm × Tier × OOD).") |>
    kableExtra::kable_styling(latex_options = c("hold_position","scale_down"), font_size = 9) |>
    print()
} else {
  knitr::kable(design, booktabs = TRUE, caption = "Design factors and levels (crossed: Arm × Tier × OOD).")
}
```

Randomization is implemented with a reproducible script and a fixed seed recorded in the registry; the assignment file is timestamped and stored read‑only. Allocation is concealed until check‑in, when the onsite coordinator reveals arm and task. No‑shows remain in their assigned arm for intent‑to‑treat analyses. Replacements are permitted only before the event begins and are re‑randomized using the same stratum‑specific seed. The same concealment and documentation protocol applies to inside/outside assignments. Any late swaps or deviations are logged prior to accessing outcomes.

We plan to enroll roughly 300 participants across multiple events. For power, we assume a plausible tier composition (more undergraduates than postdocs/professors) rather than equal counts by tier. Simulations suggest that, with baseline success gaps between undergraduates and professors of 15–20 percentage points and AI compressing roughly 40% of that gap, we achieve at least 80% power for the vertical interactions at $\alpha$ = 0.05. For the horizontal dimension, we assume a baseline OOD penalty of 20–25 percentage points in the control arm and target detectable AI‑induced reductions of about 8–10 percentage points, while preserving the same overall sample size. Throughout, we rely on heteroskedasticity-robust standard errors. Table \ref{tab:power-assumptions} summarizes the core inputs; we will freeze any updates to these assumptions prior to registry lock.

The primary analysis set follows intent‑to‑treat principles and includes all randomized individuals with any outcome data. We do not impute outcomes. For covariates only, we handle missingness as follows: categorical covariates (tier, software, prior ChatGPT familiarity, and—where used descriptively—participant and task discipline) gain an explicit “Missing” category if needed; continuous covariates (years of coding) use within‑stratum median imputation with a missingness indicator. We report outcome and covariate missingness by arm and verify robustness to listwise deletion.

```{r tbl-power, results='asis'}
lines <- c()
lines <- c(lines, '\\begin{table}[H]')
lines <- c(lines, '  \\centering')
lines <- c(lines, '  \\caption{Prospective power and design inputs (pre-lock)}')
lines <- c(lines, '  \\label{tab:power-assumptions}')
lines <- c(lines, '  {\\scriptsize')
lines <- c(lines, '  \\begin{tabular}{ll}')
lines <- c(lines, '  \\toprule')
lines <- c(lines, '  \\textbf{Quantity} & \\textbf{Value} \\\\')
lines <- c(lines, '  \\midrule')
lines <- c(lines, '  Participants (N) & 300 (approximate) \\\\')
lines <- c(lines, '  Tier composition (assumed) & UG 35--40\\%, Graduate 35--40\\%, Professor/Researcher 20--25\\% \\\\')
lines <- c(lines, '  Discipline composition & $\\approx$50\\% Econ, 25\\% PolSci, 25\\% Psych \\\\')
lines <- c(lines, '  Allocation & 1:1 within tier (AI vs Control) \\\\')
lines <- c(lines, '  Inside vs Outside discipline & Undergrads always inside \\\\')
lines <- c(lines, '  Variance estimator &  heteroskedasticity-robust SEs \\\\')
lines <- c(lines, '  \\midrule')
lines <- c(lines, '  Vertical: control success (UG; P) & 40\\% ; 55--60\\% (assumed) \\\\')
lines <- c(lines, '  Vertical: baseline gap (UG vs Prof) & 15--20 pp (assumed) \\\\')
lines <- c(lines, '  Vertical: detectable compression & $\\approx$ 40\\% of gap @ 80\\% power, $\\alpha=0.05$ \\\\')
lines <- c(lines, '  Vertical: baseline major/minor error rates & Major 0.40--0.50; Minor 0.80--1.00 (counts per participant, illustrative) \\\\')
lines <- c(lines, '  \\midrule')
lines <- c(lines, '  Horizontal: OOD penalty (control) & 20--25 pp (assumed) \\\\')
lines <- c(lines, '  Horizontal: detectable AI reduction (Success) & $\\approx$ 8--10 pp @ 80\\% power, $\\alpha=0.05$ \\\\')
lines <- c(lines, '  Horizontal: detectable AI reduction (Major errors) & $\\approx$ 0.15--0.20 fewer errors @ 80\\% power (illustrative) \\\\')
lines <- c(lines, '  Non-coding baseline (Appropriate; Overall 0--5) & 45--55\\% ; mean 3.0--3.3 (assumed) \\\\')
lines <- c(lines, '  Robustness execution baseline & 55--60\\% ≥1 check; 30--35\\% ≥2 checks (assumed) \\\\')
lines <- c(lines, '  \\bottomrule')
lines <- c(lines, '  \\end{tabular}')
lines <- c(lines, '  }')
lines <- c(lines, '\\end{table}')
cat(paste(lines, collapse='\n'))
```

# Outcomes and Measurement

Participants receive a discipline‑tagged article, a standardized instruction sheet, and a data/code package (as available). They are asked to reproduce a pre‑specified focal result and to document their workflow. At the end of the seven‑hour window, participants submit two items: (i) the standardized Excel workbook (`Reports/Replication_Log_Referee_Template.xlsx`) completed with all required tabs, and (ii) a short post-event SurveyMonkey questionnaire covering arm assignment, perceived impact of AI access, usage intensity (AI arm only), perceived disadvantage (human-only arm), and open-ended reflections on helpful/missing AI features. No additional files are required. Human graders (blinded to treatment and discipline) later use the Excel workbook and survey responses to score success, error detection, and referee outcomes using the pre-defined rubric. The AI referee prompt in the Appendix mirrors the human rubric and is applied to the narrative fields captured in the workbook.

We standardize the workflow with the same Excel template. Sheet `00_Main` captures session metadata (participant name, article identifiers, software, event, discipline tags, and the out-of-discipline flag). Sheet `01_CodingErrors` provides the structured log for major/minor issues, including timestamp, affected element, narrative justification, evidence pointer, and a minor/major self-classification toggle (although it serves only as a reference). Sheet `02_Computation` records whether the focal result was reproduced and when. Sheet `03_Robustness` allocates parallel columns for up to two robustness checks—each with motives, specification changes, original versus reproduced estimates, and interpretation—enforcing the “maximum two” rule in the protocol. Sheet `04_Referee_Report` houses the scored rubric (appropriateness plus narrative fields that feed both human and AI assessments). The post-event SurveyMonkey form captures the additional perception questions listed above; responses are linked back to the workbook for grading and descriptive summaries. The instrument records name and email, asks participants to confirm their arm assignment, and then branches: AI-assisted participants rate usage intensity on a 0–100 slider, report perceived performance impact on a –100 to 100 slider, list helpful AI features, and indicate which AI tools were used (multi-select, with an open ‘Other’ option). Human-only participants rate perceived performance impact without AI on the same –100 to 100 slider and describe which AI features they would have found most helpful. All respondents can provide open-ended comments.

We organize outcomes into two families that mirror our hypotheses: coding skills and non-coding skills. Coding skills track whether participants reproduced the focal result and how effectively they detected major or minor coding errors. Non-coding skills cover referee-style communication (appropriateness and overall scores from both human and AI judges) together with robustness behavior (whether participants proposed and implemented at least one or two qualifying checks). These measures together characterize both the technical and communicative dimensions of performance.

We also record a small set of supplementary variables for context—such as the treatment-by-event-order interaction that captures learning dynamics—and retain self-reported years of coding and prior AI usage as moderators (entering as covariates in the main models). The table below consolidates definitions, types, and assessment sources for quick reference; all analyses and figures use these exact definitions.

```{r tbl-outcomes, results='asis'}
suppressPackageStartupMessages({library(dplyr)})

# Prefer kableExtra for PDF-friendly column widths; fall back to knitr::kable
has_kx <- requireNamespace("kableExtra", quietly = TRUE)

outcomes <- tibble::tribble(
  ~Family,           ~Outcome,                        ~Type,        ~Measurement_Assessment,
  "Coding skill",   "Success",                       "Binary",     "Core result reproduced by endline (yes/no).",
  "Coding skill",   "Error detection — major",       "Count",      "Number of major coding errors correctly identified (pre-defined rubric).",
  "Coding skill",   "Error detection — minor",       "Count",      "Number of minor coding errors correctly identified (pre-defined rubric).",
  "Non-coding skill","Referee appropriateness (human)", "Binary",     "Appropriate vs. Not appropriate; human judges (blinded).",
  "Non-coding skill","Referee overall 0–5 (human)",   "0–5",        "Holistic 0–5 assessment; human score averaged across judges.",
  "Non-coding skill","Referee appropriateness (AI)",  "Binary",     "Appropriate vs. Not appropriate; AI judge using mirrored rubric.",
  "Non-coding skill","Referee overall 0–5 (AI)",      "0–5",        "Holistic 0–5 assessment from AI judge using mirrored rubric.",
  "Non-coding skill","Robustness planned ≥1",         "Binary",     "Indicator for proposing at least one qualifying robustness check (rubric-based).",
  "Non-coding skill","Robustness planned ≥2",         "Binary",     "Indicator for proposing at least two qualifying robustness checks.",
  "Non-coding skill","Robustness implemented ≥1",     "Binary",     "Indicator for executing at least one qualifying robustness check with code and outputs.",
  "Non-coding skill","Robustness implemented ≥2",     "Binary",     "Indicator for executing at least two qualifying robustness checks with code and outputs.",
  "Supplementary",  "Learning (treatment × event order)", "Interaction","Assesses whether treatment effects evolve across repeated events.",
  "Heterogeneity",      "Years of coding",                "Continuous", "Self-reported; used for heterogeneity analysis (not an outcome).",
  "Heterogeneity",      "Prior AI usage",                 "Categorical", "Self-reported; included as precision control and used for heterogeneity analysis (not an outcome)."
)

if (has_kx) {
  kableExtra::kbl(outcomes, booktabs = TRUE, longtable = TRUE,
                  caption = "Outcomes and measurements (coding and non-coding skill families).",
                  col.names = c("Family","Outcome","Type","Measurement / Assessment")) |>
    kableExtra::kable_styling(latex_options = c("hold_position","scale_down"), font_size = 9) |>
    kableExtra::column_spec(1, width = "2.2cm") |>
    kableExtra::column_spec(2, width = "3.6cm") |>
    kableExtra::column_spec(3, width = "2.3cm") |>
    kableExtra::column_spec(4, width = "7.0cm") |>
    kableExtra::collapse_rows(columns = 1, valign = "top") |>
    print()
} else {
  # Fallback: basic kable (may be wider)
  knitr::kable(outcomes, booktabs = TRUE,
               caption = "Outcomes and measurements (coding and non-coding skill families).",
               col.names = c("Family","Outcome","Type","Measurement / Assessment"))
}
```

We pre-define classification of “major” versus “minor” errors and use a standardized grading rubric. As a preview of magnitudes, the table below (Table \ref{tab:branches}) reports simple means (and standard deviations) by arm, together with Welch tests for the difference in means. Figures then visualize the core outcomes that the analysis will focus on (Figure \ref{fig:outcomes}).

The figures referenced here are predefined mock‑ups of the panels we will produce once data are available; they are rendered using synthetic or placeholder data solely to illustrate the visual layout and expected content. Their purpose is to communicate the intended presentation of our preregistered estimands rather than to reveal any substantive pattern at this stage.

```{r tbl-branches, results='asis'}
suppressPackageStartupMessages({library(dplyr)})

ind <- readRDS('data/AI individuals.rds')
ind <- ind %>% mutate(branch = factor(as.character(branch), levels = c('Human-Only','AI-Assisted')))

# Utility formatters
fmt <- function(x) sprintf('%0.3f', x)
fmt_p <- function(p) {
  if (!is.finite(p)) return('-')
  if (p < 0.001) return('\\textless0.001')
  sprintf('%0.3f', p)
}
cell_ms <- function(m, s) sprintf('\\shortstack{%s\\\\(%s)}', fmt(m), fmt(s))
cell_dp <- function(d, p) sprintf('\\shortstack{%s\\\\\\relax[%s]}', fmt(d), fmt_p(p))

# Welch test and difference (group1 − group2)
pw <- function(x, g1, g2) {
  x1 <- x[ind$branch == g1]
  x2 <- x[ind$branch == g2]
  d  <- mean(x1, na.rm = TRUE) - mean(x2, na.rm = TRUE)
  p  <- tryCatch(t.test(x1, x2)$p.value, error = function(e) NA_real_)
  list(d = d, p = p)
}

# Outcomes (Tables 4–5 scope)
vars <- list(
  'Reproduction'                              = 'reproduction_i',
  'Number of minor errors'                    = 'minor_errors_i',
  'Number of major errors'                    = 'major_errors_i',
  'Planned ≥1 robustness check'               = 'good_checks_i',
  'Planned ≥2 robustness checks'              = 'two_good_checks_i',
  'Implemented ≥1 robustness check'           = 'implemented_check_1_i',
  'Implemented ≥2 robustness checks'          = 'implemented_check_2_i',
  'Appropriate (human)'                       = 'referee_app_human_i',
  'Overall 0–5 (human)'                       = 'referee_overall_human_i',
  'Appropriate (AI)'                          = 'referee_app_ai_i',
  'Overall 0–5 (AI)'                          = 'referee_overall_ai_i'
)

fallback_h <- if ('referee_score_human_i' %in% names(ind)) ind$referee_score_human_i else rep(0, nrow(ind))
fallback_ai <- if ('referee_score_ai_i' %in% names(ind)) ind$referee_score_ai_i else rep(0, nrow(ind))
for (nm in unique(unname(unlist(vars)))) {
  if (!nm %in% names(ind)) {
    if (grepl('_human_', nm, fixed = TRUE)) {
      ind[[nm]] <- fallback_h
    } else if (grepl('_ai_', nm, fixed = TRUE)) {
      ind[[nm]] <- fallback_ai
    } else {
      ind[[nm]] <- rep(NA_real_, nrow(ind))
    }
  }
}

lines <- c()
lines <- c(lines, '\\begin{table}[H]')
lines <- c(lines, '      \\centering')
lines <- c(lines, '      \\caption{Comparison of Human-Only and AI-Assisted Metrics}')
lines <- c(lines, ' \\label{tab:branches}')
lines <- c(lines, ' {\\scriptsize')
lines <- c(lines, '')
lines <- c(lines, ' \\begin{tabular}{lccc}')
lines <- c(lines, ' \\toprule')
lines <- c(lines, ' \\textbf{Variable} & \\textbf{Human-Only} & \\textbf{AI-Assisted} & \\textbf{\\shortstack{Human-Only\\\\vs\\\\AI-Assisted}}\\\\')
lines <- c(lines, ' \\midrule')

for (lab in names(vars)) {
  v <- vars[[lab]]
  x <- ind[[v]]
  mH <- mean(x[ind$branch == 'Human-Only'],  na.rm = TRUE)
  sH <- sd(  x[ind$branch == 'Human-Only'],  na.rm = TRUE)
  mA <- mean(x[ind$branch == 'AI-Assisted'],  na.rm = TRUE)
  sA <- sd(  x[ind$branch == 'AI-Assisted'],  na.rm = TRUE)
  d_HA <- pw(x, 'Human-Only', 'AI-Assisted')
  line <- sprintf('%s & %s & %s & %s \\\\',
                  lab,
                  cell_ms(mH, sH), cell_ms(mA, sA),
                  cell_dp(d_HA$d, d_HA$p))
  lines <- c(lines, line, ' [1em]')
}
lines <- c(lines, ' \\bottomrule')
lines <- c(lines, ' \\end{tabular}')
lines <- c(lines, ' \\vspace{0.25em}')
lines <- c(lines, ' \\parbox{0.9\\textwidth}{\\scriptsize \\textit{Note:} Columns 2--3 present means and standard deviations in parentheses for the two arms; column 4 presents the difference in means (Human-Only − AI-Assisted) and two-sided Welch p-values in brackets.}')
lines <- c(lines, ' }')
lines <- c(lines, ' \\end{table}')

cat(paste(lines, collapse='\n'))
```

```{r tbl-discipline-balance, results='asis'}
suppressPackageStartupMessages({library(dplyr)})
ind <- tryCatch(readRDS('data/AI individuals.rds'), error=function(e) NULL)
if (!is.null(ind) && all(c('participant_discipline','task_discipline') %in% names(ind))) {
  ind <- ind %>% mutate(
    arm = factor(ifelse(treatment==1,'AI-Assisted','Human-Only'), levels=c('Human-Only','AI-Assisted')),
    participant_discipline = factor(participant_discipline, levels=c('Economics','Political Science','Psychology')),
    task_discipline = factor(task_discipline, levels=c('Economics','Political Science','Psychology')),
    out_of_disc_i = as.integer(!is.na(participant_discipline) & !is.na(task_discipline) & participant_discipline != task_discipline)
  )
  share <- function(x) mean(x, na.rm=TRUE)
  comp <- ind %>% group_by(arm) %>% summarize(
    Economics = share(participant_discipline=='Economics'),
    `Political Science` = share(participant_discipline=='Political Science'),
    Psychology = share(participant_discipline=='Psychology'),
    `OOD share` = share(out_of_disc_i==1), .groups='drop')
  fmt <- function(p) sprintf('%0.1f\\%%', 100*p)
  lines <- c()
  lines <- c(lines, '\\begin{table}[H]')
  lines <- c(lines, '  \\centering')
  lines <- c(lines, '  \\caption{Discipline composition and OOD share by arm}')
  lines <- c(lines, '  \\label{tab:discipline-balance}')
  lines <- c(lines, '  {\\scriptsize')
  lines <- c(lines, '  \\begin{tabular}{lcc}')
  lines <- c(lines, '  \\toprule')
  lines <- c(lines, '  \\textbf{Quantity} & \\textbf{Human-Only} & \\textbf{AI-Assisted} \\\\')
  lines <- c(lines, '  \\midrule')
  lines <- c(lines, sprintf('  Economics & %s & %s \\\\', fmt(comp$Economics[comp$arm=='Human-Only']), fmt(comp$Economics[comp$arm=='AI-Assisted'])))
  lines <- c(lines, sprintf('  Political Science & %s & %s \\\\', fmt(comp$`Political Science`[comp$arm=='Human-Only']), fmt(comp$`Political Science`[comp$arm=='AI-Assisted'])))
  lines <- c(lines, sprintf('  Psychology & %s & %s \\\\', fmt(comp$Psychology[comp$arm=='Human-Only']), fmt(comp$Psychology[comp$arm=='AI-Assisted'])))
  lines <- c(lines, sprintf('  OOD share & %s & %s \\\\', fmt(comp$`OOD share`[comp$arm=='Human-Only']), fmt(comp$`OOD share`[comp$arm=='AI-Assisted'])))
  lines <- c(lines, '  \\bottomrule')
  lines <- c(lines, '  \\end{tabular}')
  lines <- c(lines, '  }')
  lines <- c(lines, '\\end{table}')
  cat(paste(lines, collapse='\n'))
}
```

```{r fig-outcomes, fig.cap='Coding skill outcomes (levels): reproduction and coding errors. Notes: Panels display reproduction rates (raw) alongside major and minor error counts. \\label{fig:outcomes}', fig.show='hold', out.width='45%', fig.pos='H'}
# Panels: reproduction (raw), major errors (raw), minor errors (raw)
files <- c('reproduction rates (raw).pdf',
           'major errors (raw).pdf',
           'minor errors (raw).pdf')
knitr::include_graphics(ensure_sanitized_figures(files))
```

\FloatBarrier

<!-- Consolidated in fig-outcomes; removed separate error panel to reduce duplication. -->

```{r fig-success-arm-ood, fig.cap='Success by arm and outside-of-discipline (OOD) status with 95% CIs. Notes: Points and whiskers from binomial proportion tests. \\label{fig:success-arm-ood}', fig.pos='H'}
suppressPackageStartupMessages({library(dplyr); library(ggplot2)})
ind <- tryCatch(readRDS('data/AI individuals.rds'), error=function(e) NULL)
if (!is.null(ind) && all(c('participant_discipline','task_discipline') %in% names(ind))) {
  dat <- ind %>% mutate(
    arm = factor(ifelse(treatment==1,'AI-Assisted','Human-Only'), levels=c('Human-Only','AI-Assisted')),
    out_of_disc = factor(ifelse(participant_discipline!=task_discipline,'Outside','Inside'), levels=c('Inside','Outside'))
  )
  summ <- dat %>% group_by(arm, out_of_disc) %>% summarise(
    p = mean(reproduction_i, na.rm=TRUE),
    n = sum(!is.na(reproduction_i)), .groups='drop') %>%
    rowwise() %>% mutate(
      lwr = suppressWarnings(binom.test(round(p*n), n)$conf.int[1]),
      upr = suppressWarnings(binom.test(round(p*n), n)$conf.int[2])
    ) %>% ungroup()
  ggplot(summ, aes(x=out_of_disc, y=p, color=arm)) +
    geom_point(position=position_dodge(width=0.4)) +
    geom_errorbar(aes(ymin=lwr, ymax=upr), width=0.2, position=position_dodge(width=0.4)) +
    scale_y_continuous(labels=scales::percent_format(accuracy=1)) +
    labs(x='Task relative to participant discipline', y='Success rate', color='Arm') +
    scale_color_manual(values = c('Human-Only' = '#333333', 'AI-Assisted' = '#1f77b4')) +
    theme_minimal(base_size=12) + theme(legend.position='bottom')
}
```

## Grading Rubric (Operational Definitions)

Coding skills
- Success (binary): “Yes” when the participant’s final output matches the pre‑specified focal result within documented tolerance (numeric threshold or visual equivalence for figures), with code that runs cleanly to produce the output. Partial or alternative results do not count.
- Major error (count): Any coding/data/model issue that, once corrected, changes the focal result’s sign, statistical significance, or substantive interpretation; for example a wrong sample filter or an omitted transformation that alters the reported effect size. Issues that could matter but do not demonstrably affect the focal estimate are coded as minor.
- Minor error (count): Formatting, non‑substantive code, or reproducibility issues that do not change the focal result. Examples: mislabeled axes; non‑deterministic seed; inefficient code without impact on estimate.

Non-coding skills
- Appropriateness (binary): “Appropriate” when the report accurately identifies whether the reproduced evidence supports the original claim, flags substantive issues if present.
- Overall (0–5): Holistic assessment of the referee report quality (not a mechanical average of component scores). Anchors follow the referee rubric’s 0–5 scale, where 0 denotes missing/incorrect and 5 denotes comprehensive, well-argued feedback.
- Planned robustness checks: Indicators capturing whether the participant proposes at least one or at least two robustness checks that satisfy the prespecified rubric (clear motivation, not already implemented by authors, feasibility, focuses on validity of the empirical strategy).
- Implemented robustness checks: Indicators capturing whether the participant successfully executes at least one or at least two of the planned robustness checks, documented with resulting estimates.

Human graders apply these definitions blind to treatment and discipline; disagreements are reconciled via consensus. The same rubric is prompted to the AI judge for the AI‑referee outcomes.

```{r tbl-branches-horizontal, results='asis'}
suppressPackageStartupMessages({library(dplyr)})
ind <- tryCatch(readRDS('data/AI individuals.rds'), error=function(e) NULL)
if (!is.null(ind) && all(c('participant_discipline','task_discipline') %in% names(ind))) {
  dat <- ind %>% mutate(
    arm = factor(ifelse(treatment==1,'AI-Assisted','Human-Only'), levels=c('Human-Only','AI-Assisted')),
    ood = factor(ifelse(participant_discipline!=task_discipline,'Outside','Inside'), levels=c('Inside','Outside'))
  )
  fmt <- function(x) sprintf('%0.3f', x)
  fmt_p <- function(p) { if(!is.finite(p)) return('-'); if(p<0.001) return('\\textless0.001'); sprintf('%0.3f', p) }
  cell_ms <- function(m,s) sprintf('\\shortstack{%s\\\\(%s)}', fmt(m), fmt(s))
  cell_dp <- function(d,p) sprintf('\\shortstack{%s\\\\\\relax[%s]}', fmt(d), fmt_p(p))
  mk <- function(vlab, var){
    x <- dat[[var]]
    m_HI <- mean(x[dat$arm=='Human-Only' & dat$ood=='Inside'], na.rm=TRUE)
    s_HI <- sd(  x[dat$arm=='Human-Only' & dat$ood=='Inside'], na.rm=TRUE)
    m_HO <- mean(x[dat$arm=='Human-Only' & dat$ood=='Outside'], na.rm=TRUE)
    s_HO <- sd(  x[dat$arm=='Human-Only' & dat$ood=='Outside'], na.rm=TRUE)
    m_AI <- mean(x[dat$arm=='AI-Assisted' & dat$ood=='Inside'], na.rm=TRUE)
    s_AI <- sd(  x[dat$arm=='AI-Assisted' & dat$ood=='Inside'], na.rm=TRUE)
    m_AO <- mean(x[dat$arm=='AI-Assisted' & dat$ood=='Outside'], na.rm=TRUE)
    s_AO <- sd(  x[dat$arm=='AI-Assisted' & dat$ood=='Outside'], na.rm=TRUE)
    d_H  <- m_HO - m_HI; p_H <- tryCatch(t.test(x ~ ood, subset = dat$arm=='Human-Only')$p.value, error=function(e) NA_real_)
    d_A  <- m_AO - m_AI; p_A <- tryCatch(t.test(x ~ ood, subset = dat$arm=='AI-Assisted')$p.value, error=function(e) NA_real_)
    sprintf('%s & %s & %s & %s & %s & %s & %s \\\\ [1em]', vlab,
            cell_ms(m_HI,s_HI), cell_ms(m_HO,s_HO), cell_ms(m_AI,s_AI), cell_ms(m_AO,s_AO),
            cell_dp(d_H,p_H), cell_dp(d_A,p_A))
  }
  lines <- c()
  lines <- c(lines, '\\begin{table}[H]')
  lines <- c(lines, '  \\centering')
  lines <- c(lines, '  \\caption{Descriptives by arm and OOD status (means and SD).}')
  lines <- c(lines, '  \\label{tab:branches-horizontal}')
  lines <- c(lines, '  {\\scriptsize')
  lines <- c(lines, '  \\begin{tabular}{lcccccc}')
  lines <- c(lines, '  \\toprule')
  lines <- c(lines, '  \\textbf{Variable} & \\textbf{HO Inside} & \\textbf{HO Outside} & \\textbf{AI Inside} & \\textbf{AI Outside} & \\textbf{HO (Out-In)} & \\textbf{AI (Out-In)} \\\\')
  lines <- c(lines, '  \\midrule')
  rows <- c(
    mk('Reproduction','reproduction_i'),
    mk('Minor errors','minor_errors_i'),
    mk('Major errors','major_errors_i')
  )
  lines <- c(lines, rows)
  lines <- c(lines, '  \\bottomrule')
  lines <- c(lines, '  \\end{tabular}')
  lines <- c(lines, '  }')
  lines <- c(lines, '\\end{table}')
  cat(paste(lines, collapse='\n'))
}
```

We will interpret Figure \ref{fig:outcomes} as the primary visualization of coding outcomes (reproduction and error rates) with accompanying usage context. Difference-style and cumulative milestone panels are omitted to focus attention on the preregistered estimands.


# Controls (Covariates and Stratification)

Table \ref{tab:controls} summarizes the covariates and fixed effects we pre‑specify for precision and design alignment. We use a compact, stable set that mirrors the randomization scheme and absorbs systematic heterogeneity without overfitting.

We include a small, pre‑specified set of controls to improve precision and absorb systematic differences that are not of direct interest. Stratification by expertise tier (Undergraduate, Graduate, Professor/Researcher) reflects our design and is included as dummies in the main specification so that treatment effects are identified within tier; the interaction with treatment captures heterogeneous effects along the expertise ladder. Event and article fixed effects absorb site‑ and task‑specific differences. Software indicators (R/Stata/Python) capture baseline workflow differences across toolchains. Finally, self‑reported years of coding and prior AI familiarity improve precision and help stabilize estimates across events. Years of coding also serves as the pre‑specified continuous moderator in secondary analyses; the resulting interaction terms do not alter the main ITT estimands.

```{r tbl-controls, results='asis'}
suppressPackageStartupMessages(library(dplyr))
has_kx <- requireNamespace("kableExtra", quietly = TRUE)

controls <- tibble::tribble(
  ~Variable,          ~Role,            ~Type,         ~Coding_or_Levels,                               ~Notes,
  "Expertise tier",   "Stratification; control", "Categorical", "Undergrad, Graduate, Professor/Researcher", "Tier dummies in main specs; Graduate = MA/PhD, Professor/Researcher = postdoc/faculty; interacted with AI (heterogeneity).",
  "Event",            "Fixed effect",  "Categorical", "One FE per event",                              "Absorbs site/time differences (not a parameter of interest).",
  "Article",          "Fixed effect",  "Categorical", "One FE per task/article",                      "Absorbs task-specific difficulty/fit (not a parameter).",
  "Software",         "Control",       "Categorical", "R, Stata, Python",                              "Preferred software indicator (workflow baseline).",
  "Years of coding",  "Control; moderator", "Continuous",  "Self-reported years",                        "Improves precision; interacted with AI in supplementary analyses (yrs×AI).",
  "Prior ChatGPT familiarity", "Control",   "Categorical", "None, Some, Heavy",                             "Self‑reported familiarity with ChatGPT/AI tools.",
  "Variance estimator", "Estimation setting", "—",           " heteroskedasticity-robust SEs",             "Used for all reported models."
)

if (has_kx) {
  kableExtra::kbl(controls, booktabs = TRUE, longtable = FALSE,
                  caption = "Controls, fixed effects, and moderators (prespecified). \\label{tab:controls}") |>
    kableExtra::kable_styling(latex_options = c("hold_position","scale_down"), font_size = 8) |>
    kableExtra::column_spec(1, width = "2.2cm") |>
    kableExtra::column_spec(2, width = "2.6cm") |>
    kableExtra::column_spec(3, width = "1.6cm") |>
    kableExtra::column_spec(4, width = "3.8cm") |>
    kableExtra::column_spec(5, width = "4.0cm") |>
    print()
} else {
  knitr::kable(controls, booktabs = TRUE,
               caption = "Controls, fixed effects, and moderators (prespecified). \\label{tab:controls}")
}
```

\FloatBarrier

<!-- balance table moved into Statistical Analysis Plan -->

\FloatBarrier

# Statistical Analysis Plan

This section lays out our estimands and modeling approach for the two preregistered dimensions of interest—vertical (expertise tier) and horizontal (out‑of‑discipline, OOD)—and clarifies how outcome types map to link functions and fixed‑effect structure. We begin with intent‑to‑treat (ITT) specifications and then describe the interaction terms that capture compression: AI × tier for vertical and AI × OOD for horizontal. Event×article fixed effects absorb site/tool and task heterogeneity; all models report  heteroskedasticity-robust standard errors.

Let $Y_i$ denote a pre‑specified outcome (reproduction success; counts of minor or major coding errors; referee appropriateness; referee overall score; robustness-check indicators). We estimate ITT effects in a tier‑interaction framework that allows the AI effect to vary with expertise. Formally,

$$
Y_i \,=\, \beta_0 \, + \, \beta_1 A_i \, + \, \sum_s \gamma_s\,\mathbb{1}\{\mathrm{Tier}_i=s\} \, + \, \sum_s \delta_s\, A_i\,\mathbb{1}\{\mathrm{Tier}_i=s\} \, + \, X_i'\theta \, + \, \lambda_{e(i)\times s(i)} \, + \, \varepsilon_i,
$$

where $A_i$ is the treatment indicator, $\lambda_{e(i)\times s(i)}$ are event-by-software fixed effects, and $X_i$ collects the prespecified controls (years of coding and prior AI familiarity). Binary outcomes (reproduction, referee appropriateness, robustness indicators) are estimated with probit models; continuous outcomes (overall 0–5 scores) use OLS; and counts (minor/major errors) use a Poisson model:

$$
\mathbb{E}[Y_i\mid\cdot] \,=\, \exp\!\left( \beta_0 + \beta_1 A_i + \sum_s \gamma_s\,\mathbb{1}\{\mathrm{Tier}_i=s\} + \sum_s \delta_s\, A_i\,\mathbb{1}\{\mathrm{Tier}_i=s\} + X_i'\theta + \lambda_{e(i)\times s(i)} \right).
$$

Horizontal compression estimands. Let $D_i$ denote an indicator for receiving a task outside the participant’s primary discipline (OOD). We test whether AI reduces the OOD penalty by augmenting the main models with $D_i$ and $A_i\times D_i$: $Y_i = \cdots + \beta_2 D_i + \beta_3 (A_i\times D_i) + \varepsilon_i$. The coefficient $\beta_3$ captures horizontal compression (a smaller penalty, or a gain, when outside). Article indicators absorb discipline effects because tasks carry a single discipline tag.

Interpretation and outcome‑specific notes. In all tables, the “AI‑Assisted” row reports the ITT contrast for participants in the baseline tier or inside‑discipline case; the interaction rows report how that contrast varies along tiers (vertical) or OOD (horizontal). For counts, we check over‑dispersion (deviance/df) and, if material, re‑estimate with a negative binomial link as a robustness check.

To assess the success of randomization and support the modeling choices, we report balance on all individual-level controls used in the specification. The table below (Table \ref{tab:balance}) shows means (standard deviations) by arm and Welch tests for the difference in means.

```{r tbl-balance, results='asis', echo=FALSE}
suppressPackageStartupMessages({library(dplyr)})

ind <- readRDS('data/AI individuals.rds')
ind <- ind %>% mutate(
  arm = factor(ifelse(treatment==1, 'AI-Assisted','Human-Only'), levels = c('Human-Only','AI-Assisted')),
  tier = factor(tier, levels = c('UG','MA','PhD','PD','P')),
  tier_3 = factor(case_when(
    tier == 'UG' ~ 'UG',
    tier %in% c('MA','PhD') ~ 'GR',
    tier %in% c('PD','P') ~ 'PR',
    TRUE ~ NA_character_
  ), levels = c('UG','GR','PR')),
  software3 = factor(preferred_software, levels = c('R','Stata','Python')),
  prior_gpt_familiarity = factor(prior_gpt_familiarity, levels = c('None','Some','Heavy'))
)

fmt <- function(x) sprintf('%0.3f', x)
fmt_p <- function(p) { if (!is.finite(p)) return('-'); if (p < 0.001) return('\\textless0.001'); sprintf('%0.3f', p) }
cell_ms <- function(m, s) sprintf('\\shortstack{%s\\\\(%s)}', fmt(m), fmt(s))
cell_dp <- function(d, p) sprintf('\\shortstack{%s\\\\\\relax[%s]}', fmt(d), fmt_p(p))

mk_row <- function(x, label){
  xH <- x[ind$arm=='Human-Only']
  xA <- x[ind$arm=='AI-Assisted']
  mH <- mean(xH, na.rm = TRUE); sH <- sd(xH, na.rm = TRUE)
  mA <- mean(xA, na.rm = TRUE); sA <- sd(xA, na.rm = TRUE)
  d  <- mH - mA
  p  <- tryCatch(t.test(xH, xA)$p.value, error = function(e) NA_real_)
  sprintf('%s & %s & %s & %s \\\\ [1em]', label, cell_ms(mH,sH), cell_ms(mA,sA), cell_dp(d,p))
}

# display labels for the three-tier grouping
lab_tier3 <- c(UG = 'Undergraduate', GR = 'Graduate', PR = 'Professor/Researcher')

lines <- c()
lines <- c(lines, '\\begin{table}[H]')
lines <- c(lines, '  \\centering')
lines <- c(lines, '  \\caption{Balance of Participant Characteristics by Arm}')
lines <- c(lines, '  \\label{tab:balance}')
lines <- c(lines, '  {\\scriptsize')
lines <- c(lines, '  \\begin{tabular}{lccc}')
lines <- c(lines, '  \\toprule')
lines <- c(lines, '  \\textbf{Variable} & \\textbf{Human-Only} & \\textbf{AI-Assisted} & \\textbf{\\shortstack{Human-Only\\\\vs\\\\AI-Assisted}} \\\\')
lines <- c(lines, '  \\midrule')

# Years of coding (continuous)
lines <- c(lines, mk_row(ind$years_coding, 'Years of coding'))

# Tier (three dummies)
for (tt in levels(ind$tier_3)) {
  if (!is.na(tt)) {
    x <- as.integer(ind$tier_3 == tt)
    lines <- c(lines, mk_row(x, sprintf('Tier: %s', lab_tier3[[tt]])))
  }
}

# Software (three dummies)
for (sw in levels(ind$software3)) {
  if (!is.na(sw)) {
    x <- as.integer(ind$software3 == sw)
    lines <- c(lines, mk_row(x, sprintf('Software: %s', sw)))
  }
}

# Prior ChatGPT familiarity (three dummies)
for (ff in levels(ind$prior_gpt_familiarity)) {
  if (!is.na(ff)) {
    x <- as.integer(ind$prior_gpt_familiarity == ff)
    lines <- c(lines, mk_row(x, sprintf('Prior ChatGPT familiarity: %s', ff)))
  }
}

lines <- c(lines, '  \\bottomrule')
lines <- c(lines, '  \\end{tabular}')
lines <- c(lines, '  \\vspace{0.25em}')
lines <- c(lines, '  \\parbox{0.9\\textwidth}{\\scriptsize \\textit{Note:} Means and standard deviations in parentheses by arm; difference column shows Human-Only − AI-Assisted and two-sided Welch $p$-values in brackets. All variables are individual-level controls used in the models.}')
lines <- c(lines, '  }')
lines <- c(lines, '\\end{table}')

cat(paste(lines, collapse='\n'))
```

Supplementary analyses follow two paths. First, we replace tier dummies with a continuous moderator (years of coding) interacted with treatment to trace a dose–response. Second, within the AI arm only, we summarize SurveyMonkey responses on perceived AI usage intensity and performance impact; these descriptive measures help interpret treatment effects but do not alter the main ITT estimands. We also consider event‑order interactions to gauge learning across events.

# Results

For clarity and symmetry, we present vertical (tier‑based) estimates first, followed by parallel horizontal (AI × out‑of‑discipline) estimates using the same outcomes and table layout. Figures and descriptives mirror this sequence.

We present results in two blocks that align with the preregistered outcome families—coding skills and non-coding skills—with vertical (tier) estimates first and horizontal (OOD) contrasts second. Throughout, standard errors rely on  heteroskedasticity‑robust corrections, and the coefficients are displayed with confidence intervals plus the pre‑specified compression test for the interaction terms.

```{r tbl-main, results='asis'}
emit_table_with_caption('output/tables/pap_coding.tex',
                        caption='Coding outcomes: vertical ITT effects with tier interactions.  heteroskedasticity-robust standard errors.',
                        label='tab:pap-coding')
```

\FloatBarrier

```{r tbl-main-horizontal, results='asis'}
emit_table_with_caption('output/tables/pap_coding_horizontal.tex',
                        caption='Coding outcomes: horizontal AI × outside-of-discipline effects.  heteroskedasticity-robust standard errors.',
                        label='tab:pap-coding-horizontal')
```

```{r tbl-referee, results='asis'}
emit_table_with_caption('output/tables/pap_noncoding.tex',
  caption='Non-coding outcomes: vertical ITT effects on referee judgments and robustness execution.  heteroskedasticity-robust standard errors.',
  label='tab:pap-noncoding')
```

```{r tbl-referee-horizontal, results='asis'}
emit_table_with_caption('output/tables/pap_noncoding_horizontal.tex',
  caption='Non-coding outcomes: horizontal AI × outside-of-discipline effects.  heteroskedasticity-robust standard errors.',
  label='tab:pap-noncoding-horizontal')
```



\FloatBarrier

\FloatBarrier

# Data Management, Documentation, and Ethics

We will publish de-identified participant-level data, code, and grading rubrics on OSF and GitHub upon acceptance (view-only earlier when necessary).

Static study materials live inside this repository and will be version-locked before launch: curated task bundles in `Papers/`, the pre-game training deck in `Pre game/`, and the reporting workbook in `Reports/`. Repository history provides provenance for any updates (for example, refreshed replication packages or revised slides).

Audio recordings and notes from the focus groups are stored on encrypted I4R drives with filenames keyed to anonymous participant IDs and session arm (AI vs. control). Transcription software with local processing is used where feasible; otherwise, trained staff transcribe manually. Only the qualitative analysis team accesses the raw audio. De-identified excerpts linked to codebook categories are released alongside the quantitative replication package after redacting personally identifiable information and any mention of other participants’ outputs.

# Appendix

## AI Referee Prompt

The AI grader receives a standalone instruction each time it evaluates a submission. The block below reproduces the full prompt, including the required output structure and scoring anchors.

**Role and objective**
You are "AI Referee," an expert evaluator for the AI Replication Games. Your task is to read a participant's workbook and produce referee-style assessments that mirror the human rubric.

**Context**
- You are judging a replication of a published empirical result.
- The participant had access to the original paper, its replication package, and a seven-hour work window.
- The materials you receive include the participant's referee report (sheet `04_Referee_Report`), their reproducibility notes and robustness log, and any supporting tables or figures they produced.

**General instructions**
- Work independently—do not infer facts that are absent from the materials.
- Keep the narrative within 1,000 words and organize it under the sections listed below; fold any code or data blockers into those sections instead of creating new headings.
- Base every judgment on the participant's reasoning and evidence; parroting the source paper without evaluation should be penalized.
- Highlight robustness checks only when they satisfy the rubric (motivation, documented specification change, comparison to the focal estimate, interpretation); otherwise describe the gap.
- Use the full 0–5 range for the overall score (no half points).
- Provide two- to four-sentence justifications for each scored dimension, citing the relevant portions of the workbook (quote, paraphrase, or reference to a table or figure).
- If critical evidence is missing, record that as a weakness, reflect it in the score, and flag it in the notes.

**Evaluation tasks**
1. Appropriateness (binary). Return "Appropriate" if the participant correctly characterizes whether the reproduced evidence supports the original claim, flags material discrepancies, and grounds the conclusion in the submitted code or outputs. Otherwise return "Not appropriate".
2. Overall assessment (0–5). Provide a holistic rating that reflects the usefulness of the referee narrative to an editor, weighing accuracy, diagnostic insight, actionable guidance, and clarity. This is not a mechanical average.

**Anchors for the 0–5 scale (task 2)**
- 0 = Missing, wholly incorrect, or incoherent.
- 1 = Substantially inaccurate, with major misunderstandings that would mislead an editor.
- 2 = Partially correct but incomplete or containing notable inaccuracies that reduce usefulness.
- 3 = Adequate: correct core points with limited depth or weak justification.
- 4 = Strong: accurate, well-supported, and actionable with only minor omissions.
- 5 = Exceptional: precise, comprehensive, and offering original insight or particularly valuable guidance.

**Report structure**
Produce prose for the following sections. Each section should be a short paragraph that references the participant's evidence and names any missing components.
- Summary — question, setting, methods, and focal result (including whether it was reproduced and any tolerance issues).
- Design and Identification — randomization, balance, attrition, and identification threats.
- Robustness — up to two checks with rationale, specification or sample changes, and how the results compare with the focal estimate; note explicitly if required elements are absent.
- Ethical and Transparency — preregistration, data access, and consent or IRB references, if any.
- Overall Assessment — recommendation to editors/authors synthesizing strengths and weaknesses.

**Output format**
Return JSON with the following fields: `appropriateness` ("Appropriate" or "Not appropriate"), `overall`, `justifications`, `report_sections`, and `notes`. The `justifications` object must contain keys for `appropriateness` and `overall`. The `report_sections` object must contain keys `summary`, `design_identification`, `robustness`, `ethical_transparency`, and `overall_assessment`. Include a single string in `notes` describing any missing information, suspected hallucinations, or uncertainties.

```json
{
  "appropriateness": "Appropriate",
  "overall": 4,
  "justifications": {
    "appropriateness": "...",
    "overall": "..."
  },
  "report_sections": {
    "summary": "...",
    "design_identification": "...",
    "robustness": "...",
    "ethical_transparency": "...",
    "overall_assessment": "..."
  },
  "notes": "..."
}
```

**Quality control**
- Double-check that every field is filled.
- If evidence is missing for a dimension:
  - set the score to the literal value `null`,
  - explain the omission in the `notes` field, and
  - flag the gap in the corresponding justification or section.
- Do not invent references or cite external material.
- Use only the materials provided with the submission.
- Return only the JSON object—no additional prose after the closing brace.


```{r tbl-years-main, results='asis'}
emit_table_with_caption('output/tables/pap_coding_years_core.tex',
  caption='Coding outcomes with years-of-coding moderator (interaction with AI).',
  label='tab:pap-coding-years')
```

```{r tbl-years-ref, results='asis'}
emit_table_with_caption('output/tables/pap_noncoding_years.tex',
  caption='Non-coding outcomes with years-of-coding moderator (interaction with AI).',
  label='tab:pap-noncoding-years')
```

\FloatBarrier

```{r tbl-usage-by-tier, results='asis'}
# Table intentionally omitted: participant usage logs are no longer collected.
```



\FloatBarrier

# References

References render from `references.bib`. We will cite prior AI Replication Games and related methodology upon registration finalization.
